# Ollama Configuration
# Copy this file to .env and update with your actual values

# Ollama server host (default: http://localhost:11434)
OLLAMA_HOST=http://localhost:11434

# Ollama model to use (default: llama3.2)
# Available models: llama3.2, llama3, mistral, codellama, etc.
# Run 'ollama list' to see available models
MODEL=llama3.2
